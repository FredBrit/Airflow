from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import json
import logging
import os
from io import StringIO
from airflow.hooks.base import BaseHook
from clickhouse_driver import Client


# ĞŸÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼
STATE_FILE = '/opt/airflow/airflow_data/xcom_dz_state.json'

def fetch_data_to_xcom(**kwargs):
    logger = logging.getLogger(__name__)
    ds = kwargs['ds']
    ti = kwargs['ti']

    logger.info(f"Processing date: {ds}")

    if ds == '2024-01-01':
        if os.path.exists(STATE_FILE):
            os.remove(STATE_FILE)
            logger.info(f"Ğ¤Ğ°Ğ¹Ğ» ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½: {STATE_FILE}")

    # ğŸ”¹ Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            prev_file_list = json.load(f)
        logger.info(f"Loaded previous file list from {STATE_FILE}")
    else:
        prev_file_list = []
        logger.info("No previous file list found (first run).")

    # ğŸ”¹ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
    api_url = 'http://158.160.116.58:4009/files/'
    response = requests.get(f"{api_url}{ds}")
    if response.status_code != 200:
        raise AirflowException(f"Request failed: {response.status_code}")

    file_path = '/opt/airflow/airflow_data/xcom_file.json'
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(response.json(), f, ensure_ascii=False, indent=2)

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    current_file_list = data['files']

    # ğŸ”¹ ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
    new_files = [f for f in current_file_list if f not in prev_file_list]
    logger.info(f"Previous files: {prev_file_list}")
    logger.info(f"Current files: {current_file_list}")
    logger.info(f"New files: {new_files}")

    # ğŸ”¹ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ² Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(current_file_list, f, ensure_ascii=False, indent=2)

    logger.info(f"Saved current file list to {STATE_FILE}")

    ti.xcom_push(
        key='prev_file',
        value=new_files
    )

    return new_files  # Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ñ‘Ñ‚ Ğ² return_value

def upload_to_clickhouse(table_name, **kwargs):
    logger = logging.getLogger(__name__)
    ti = kwargs['ti']

    # âœ… ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ new_files Ğ¸Ğ· XCom
    new_files = ti.xcom_pull(
        task_ids='fetch_data_to_xcom',
        key='return_value'
    )

    if not new_files:
        logger.warning("No new files to upload.")
        return


    # ğŸ”¹ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Airflow Connection
    conn = BaseHook.get_connection("ch_client")
    ch_client = Client(
        host=conn.host,
        port=conn.port or 9000,
        user=conn.login,
        password=conn.password,
        database=conn.schema or 'default'
    )
    
    table_exists = ch_client.execute(f"EXISTS TABLE {table_name}")[0][0]

    if not table_exists:
        ch_client.execute(f'''
        CREATE TABLE IF NOT EXISTS {table_name} (
        campaign String, 
        cost Int64, 
        date  String
        )
        ENGINE = MergeTree()
        PRIMARY KEY (date)
        ORDER BY (date)
        ''')

    endpoint_url = 'http://158.160.116.58:4009/download/'

    for file in new_files:
        file_url = f"{endpoint_url}{file}"
        response = requests.get(file_url)
        df = pd.read_csv(StringIO(response.text))

        ch_client.execute(f'INSERT INTO {table_name} VALUES', df.to_dict('records'))     

    print(f"Uploaded data for {kwargs['ds']}")
    logger.info(f"Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ±ÑƒĞ´ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: {new_files}")

    



with DAG(
    'xcom_dz_v53',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    end_date=datetime(2024, 1, 4),
    catchup=True,
    max_active_runs=1,
    tags=['XCom'],
    default_args={
        'owner': 'airflow',
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    }
) as dag:

    start = EmptyOperator(task_id='start')

    fetch_task = PythonOperator(
        task_id='fetch_data_to_xcom',
        python_callable=fetch_data_to_xcom,
    )

    upload_task = PythonOperator(
        task_id='upload_to_clickhouse',
        python_callable=upload_to_clickhouse,
        op_args = ['fedos_af_xcom']
    )

    start >> fetch_task >> upload_task
